{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  4 16:12:44 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-PCI...  Off  | 00000000:17:00.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    41W / 250W |      0MiB / 40536MiB |      0%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA A100-PCI...  Off  | 00000000:CA:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P0    36W / 250W |      0MiB / 40536MiB |      0%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pytorch torchvision -c pytorch --yes\n",
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install  torch transformers\n",
    "# !pip install torch torchvision transformers\n",
    "# !conda install pytorch torchvision  -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which pip\n",
    "# !conda install -c conda-forge python=3.7 python pip3\n",
    "# !conda install -c conda-forge transformers\n",
    "# !conda install pytorch torchvision --yes\n",
    "\n",
    "# !pip uninstall transformers --yes\n",
    "# #!pip uninstall torch --yes\n",
    "# !pip install transformers\n",
    "# !pip install torch --yes\n",
    "# # !pip install transformers --yes\n",
    "# # !pip install cython --yes\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# datase = load_dataset(\"json\", data_files=\"te.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 16:12:55.499686: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-04 16:12:55.558953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-04 16:12:58.511603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer, AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments\n",
    "import argparse\n",
    "import logging\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "import gzip\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "# from huggingface_hub import Repository\n",
    "# repo = Repository(\"gpt2-reddit\", clone_from=\"skunusot/gpt2-reddit\")\n",
    "# repo.git_pull()\n",
    "\n",
    "output_dir_main = \"t5-finetuned-reddit-model\"\n",
    "# logging.getLogger(\"transformers\").setLevel(logging.DEBUG)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_name\", default=\"t5-small\")\n",
    "parser.add_argument(\"--train_files\", required=False, nargs='+', default=['./combinedcleaned-full.jsonl.gz'])\n",
    "parser.add_argument(\"--name\", required=False, default=\"reddit\")\n",
    "parser.add_argument(\"--train_size\", default=320000, type=int)\n",
    "parser.add_argument(\"--eval_size\", default=50, type=int)\n",
    "parser.add_argument(\"--test_size\", default=80000, type=int)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open ./combinedcleanedp1.jsonl.gz\n",
      "Target: We are literally experiencing the end of the world. I'm fucking shaking. I can't go on. I'm moving to Europe where I know I'll be safe from these white supremacists.\n",
      "Input: National Suicide Prevention Hotline: 1-800-273-8255 Don't do it!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_example(s):\n",
    "        raw_example = json.loads(s)\n",
    "        return [(clean_title(raw_example['title'])), clean_title(raw_example['body'])]\n",
    "    \n",
    "def clean_title(text):\n",
    "    text = text.replace(\"&amp;\", \"&\").strip()\n",
    "    if text.startswith(\"[\"):\n",
    "        text = re.sub(\"^\\[[a-zA-Z0-9]+\\]\", \"\", text).strip()\n",
    "\n",
    "    if text.endswith(\"]\"):\n",
    "        text = re.sub(\"\\[[a-zA-Z0-9\\.]+\\]$\", \"\", text).strip()\n",
    "\n",
    "    if text.startswith(\"/r\"):\n",
    "        text = re.sub(\"^/[a-zA-Z0-9/]+[;,: \\-]+\", \"\", text).strip()\n",
    "\n",
    "    return text\n",
    "    \n",
    "class RedditTitleDataset(IterableDataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.examples = []\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with gzip.open(self.filepath, 'rt', encoding='utf-8') as fIn:\n",
    "            for line in fIn:\n",
    "                example = get_example(json.loads(line))\n",
    "                if example is not None and example[0] != '' and example[1] != '':\n",
    "                    self.examples.append(example)\n",
    "                    yield example\n",
    "\n",
    "    def get_total(self):\n",
    "        total = 0\n",
    "        with gzip.open(self.filepath, 'rt', encoding='utf-8') as fIn:\n",
    "            for line in fIn:\n",
    "                example = get_example(json.loads(line))\n",
    "                if example[0] != '' and example[1] != '':\n",
    "                    total += 1\n",
    "        \n",
    "        return total\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(args.model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[CLS]', 'eos_token': '[SEP]'})\n",
    "\n",
    "# Define data collator\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=False\n",
    "# )\n",
    "def data_collator(examples):\n",
    "#     print(len(examples))\n",
    "#     print(len(examples[0]))\n",
    "    if examples is None: \n",
    "        return None\n",
    "    targets = [row[0] for row in examples]\n",
    "    inputs = [row[1] for row in examples]\n",
    "    # print(\"targets: \")\n",
    "    # print(targets)\n",
    "    # print(\"inputs: \")\n",
    "    # print(inputs)\n",
    "    model_inputs = tokenizer(targets, inputs, padding=True, truncation=True, return_tensors='pt', pad_to_multiple_of=8)\n",
    "\n",
    "    if not model_inputs.data[\"input_ids\"] is None or len(model_inputs.data[\"input_ids\"]) == 0:\n",
    "        # print(\"model_inputs\")\n",
    "        # print(model_inputs.data[\"input_ids\"])\n",
    "        model_inputs[\"labels\"] = torch.tensor(model_inputs.data[\"input_ids\"]).to(device)\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# implement the compute matrix to be used in evaluation the model\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    rouge_score = rouge.compute(predictions=pred_str, references=label_str),\n",
    "\n",
    "    return {\"bleu\": round(corpus_bleu(pred_str, [label_str]).score, 4),\n",
    "            \"rouge\": rouge_score,\n",
    "           }\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "# Create dataset and dataloader\n",
    "reddit_dataset = RedditTitleDataset(args.train_files[0])\n",
    "reddit_dataset_iter = iter(reddit_dataset)\n",
    "# train_dataset = [next(reddit_dataset_iter) for _ in range(reddit_dataset.get_total() - args.test_size - args.eval_size -5200087)]\n",
    "\n",
    "# print(reddit_dataset.get_total())\n",
    "# reddit_dataset.get_total() - args.eval_size-100000\n",
    "train_dataset = [next(reddit_dataset_iter) for _ in range(470000)] #5200087\n",
    "eval_dataset = [next(reddit_dataset_iter) for _ in range(args.eval_size)]\n",
    "del reddit_dataset\n",
    "del reddit_dataset_iter\n",
    "\n",
    "gc.collect()\n",
    "# train_dataset = [next(reddit_dataset_iter) for _ in range(reddit_dataset.get_total() - 100)] #5200087. 2065368\n",
    "# eval_dataset = [next(reddit_dataset_iter) for _ in range(100)]\n",
    "\n",
    "\n",
    "# test_dataset = [next(reddit_dataset_iter) for _ in range(args.test_size)]    \n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target:\", eval_dataset[1][0])\n",
    "print(\"Input:\", eval_dataset[1][1])\n",
    "\n",
    "# print(\"Train dataset len:\", len(train_dataset))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/12336316/ipykernel_54490/2336566665.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_inputs[\"labels\"] = torch.tensor(model_inputs.data[\"input_ids\"]).to(device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1836' max='1836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1836/1836 1:51:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>29.835443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>19.014589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.883795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>22.667700</td>\n",
       "      <td>8.088467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>22.667700</td>\n",
       "      <td>3.615461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>22.667700</td>\n",
       "      <td>3.257490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>5.796600</td>\n",
       "      <td>2.681874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>5.796600</td>\n",
       "      <td>2.267532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>5.796600</td>\n",
       "      <td>2.081949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.278900</td>\n",
       "      <td>1.980360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.278900</td>\n",
       "      <td>1.926714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.278900</td>\n",
       "      <td>1.905979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('t5-finetuned-reddit-model/tokenizer_config.json',\n",
       " 't5-finetuned-reddit-model/special_tokens_map.json',\n",
       " 't5-finetuned-reddit-model/spiece.model',\n",
       " 't5-finetuned-reddit-model/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\")\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "def data_collator(examples):\n",
    "    if examples is None: \n",
    "        return None\n",
    "    targets = [row[0] for row in examples]\n",
    "    inputs = [row[1] for row in examples]\n",
    "    # print(\"targets: \")\n",
    "    # print(targets)\n",
    "    # print(\"inputs: \")\n",
    "    # print(inputs)\n",
    "    model_inputs = tokenizer(targets, inputs, padding=True, truncation=True, return_tensors='pt', pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "\n",
    "    if not model_inputs.data[\"input_ids\"] is None or len(model_inputs.data[\"input_ids\"]) == 0:\n",
    "        # print(\"model_inputs\")\n",
    "        # print(model_inputs.data[\"input_ids\"])\n",
    "        model_inputs[\"labels\"] = torch.tensor(model_inputs.data[\"input_ids\"]).to(device)\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir_main,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=150,\n",
    "#     save_total_limit=2,\n",
    "    learning_rate=1e-5,\n",
    "    # fp16=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    hub_strategy=\"every_save\",\n",
    "    save_strategy=\"no\",\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=\"skunusot/gpt2-reddit\",\n",
    "    hub_private_repo=True,\n",
    "    hub_token=\"whf_LbwUQBNXqnUndGiCJePZLvNzcVRQCOXtSI\",\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "from transformers import EncoderDecoderModel, T5Config, T5ForConditionalGeneration\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(args.model_name).to(device)\n",
    "model_name = \"t5-small\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "model.config = T5Config(decoder_start_token_id=tokenizer.convert_tokens_to_ids(['<pad>'])[0])\n",
    "\n",
    "encoder_max_length = 300\n",
    "decoder_max_length = 300\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"t5-finetuned-reddit-model\")\n",
    "tokenizer.save_pretrained(\"t5-finetuned-reddit-model\")\n",
    "# compute_scores(test_dataset)\n",
    "\n",
    "# def compute_scores(test_dataset):\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained(output_dir_main)\n",
    "#     model = GPT2LMHeadModel.from_pretrained(output_dir_main)\n",
    "#     model.eval()\n",
    "#     test_input_ids = tokenizer.batch_encode_plus(test_dataset, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(test_input_ids, labels=test_input_ids)\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#     # Compute the perplexity as the exponential of the cross-entropy loss\n",
    "#     perplexity = math.exp(loss)\n",
    "#     print(f\"Perplexity: {perplexity:.2f}\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /user/skunusot/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# 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\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8814cb71b78148f4b4201097249742ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f64a2dd36243059a00273954bf6f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5b94d326fc4fde992da07c8db52923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b905d1991b6a4d3496ab7c56d89649ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/12336316/ipykernel_54490/2336566665.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_inputs[\"labels\"] = torch.tensor(model_inputs.data[\"input_ids\"]).to(device)\n",
      "/projects/academic/courses/cse546s23/skunusot/anaconda3/envs/cuda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9052320718765259, 'eval_runtime': 0.7628, 'eval_samples_per_second': 131.09, 'eval_steps_per_second': 2.622, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub(\"skunusot/finetuned-reddit-t5\")\n",
    "tokenizer.push_to_hub(\"skunusot/finetuned-reddit-t5\")\n",
    "\n",
    "evaultion = trainer.evaluate()\n",
    "print(evaultion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
