{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Apr 21 19:42:08 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\r\n",
      "| N/A   36C    P0    26W / 250W |      0MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\r\n",
      "| N/A   36C    P0    25W / 250W |      0MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pytorch torchvision -c pytorch --yes\n",
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install  torch transformers\n",
    "# !pip install torch torchvision transformers\n",
    "# !conda install pytorch torchvision  -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which pip\n",
    "# !conda install -c conda-forge python=3.7 python pip3\n",
    "# !conda install -c conda-forge transformers\n",
    "# !conda install pytorch torchvision --yes\n",
    "\n",
    "# !pip uninstall transformers --yes\n",
    "# #!pip uninstall torch --yes\n",
    "# !pip install transformers\n",
    "# !pip install torch --yes\n",
    "# # !pip install transformers --yes\n",
    "# # !pip install cython --yes\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# datase = load_dataset(\"json\", data_files=\"te.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 18:05:17.455250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 18:05:23.112726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import argparse\n",
    "import logging\n",
    "from torch.utils.data import IterableDataset\n",
    "import gzip\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import os\n",
    "import re\n",
    "# from huggingface_hub import Repository\n",
    "# repo = Repository(\"gpt2-reddit\", clone_from=\"skunusot/gpt2-reddit\")\n",
    "# repo.git_pull()\n",
    "\n",
    "output_dir_main = \"./reddit-model\"\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_name\", default=\"gpt2\")\n",
    "parser.add_argument(\"--train_files\", required=False, nargs='+', default=['./combinedcleaned-full.jsonl.gz'])\n",
    "parser.add_argument(\"--name\", required=False, default=\"reddit\")\n",
    "parser.add_argument(\"--train_size\", default=320000, type=int)\n",
    "parser.add_argument(\"--eval_size\", default=50, type=int)\n",
    "parser.add_argument(\"--test_size\", default=80000, type=int)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4361fc591e6d443baaa8a4df9f5fe399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed363934cc8c4035a84d416c29e82a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b2a37ba1a0455ebf7182d82da0f0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open ./combinedcleaned-full.jsonl.gz\n",
      "Target: The inverse solubility of certain cellulosic rheology modifiers, at room temperature they are a liquid, but as you heat them up they harden and form a solid.\n",
      "Input: I've actually worked with that. I was working on a hyaluronic acid/methylcellulose hydrogel that was both thermally setting (solidifies at 37 C) and shear thinning (liquefies when pushed through a syringe). We were looking at usin it for deliverying a hydrogel scaffold into a body through a syringe, and having it solidify in place.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_example(s):\n",
    "        raw_example = json.loads(s)\n",
    "        return [(clean_title(raw_example['title'])), clean_title(raw_example['body'])]\n",
    "    \n",
    "def clean_title(text):\n",
    "    text = text.replace(\"&amp;\", \"&\").strip()\n",
    "    if text.startswith(\"[\"):\n",
    "        text = re.sub(\"^\\[[a-zA-Z0-9]+\\]\", \"\", text).strip()\n",
    "\n",
    "    if text.endswith(\"]\"):\n",
    "        text = re.sub(\"\\[[a-zA-Z0-9\\.]+\\]$\", \"\", text).strip()\n",
    "\n",
    "    if text.startswith(\"/r\"):\n",
    "        text = re.sub(\"^/[a-zA-Z0-9/]+[;,: \\-]+\", \"\", text).strip()\n",
    "\n",
    "    return text\n",
    "    \n",
    "class RedditTitleDataset(IterableDataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.examples = []\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with gzip.open(self.filepath, 'rt', encoding='utf-8') as fIn:\n",
    "            for line in fIn:\n",
    "                example = get_example(json.loads(line))\n",
    "                if example is not None and example[0] != '' and example[1] != '':\n",
    "                    self.examples.append(example)\n",
    "                    yield example\n",
    "\n",
    "    def get_total(self):\n",
    "        total = 0\n",
    "        with gzip.open(self.filepath, 'rt', encoding='utf-8') as fIn:\n",
    "            for line in fIn:\n",
    "                example = get_example(json.loads(line))\n",
    "                if example[0] != '' and example[1] != '':\n",
    "                    total += 1\n",
    "        \n",
    "        return total\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(args.model_name)\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': tokenizer.bos_token, 'eos_token': tokenizer.eos_token})\n",
    "\n",
    "# Define data collator\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=False\n",
    "# )\n",
    "def data_collator(examples):\n",
    "    # print(examples)\n",
    "    if examples is None: \n",
    "        return None\n",
    "    targets = [row[0] for row in examples]\n",
    "    inputs = [row[1] for row in examples]\n",
    "    # print(\"targets: \")\n",
    "    # print(targets)\n",
    "    # print(\"inputs: \")\n",
    "    # print(inputs)\n",
    "    model_inputs = tokenizer(targets[0], inputs[0], padding=True, truncation=True, return_tensors='pt', pad_to_multiple_of=8)\n",
    "\n",
    "    if not model_inputs.data[\"input_ids\"] is None or len(model_inputs.data[\"input_ids\"]) == 0:\n",
    "        # print(\"model_inputs\")\n",
    "        # print(model_inputs.data[\"input_ids\"])\n",
    "        model_inputs[\"labels\"] = torch.tensor(model_inputs.data[\"input_ids\"]).to(device)\n",
    "    return model_inputs\n",
    "\n",
    "    \n",
    "\n",
    "import gc\n",
    "\n",
    "# Create dataset and dataloader\n",
    "reddit_dataset = RedditTitleDataset(args.train_files[0])\n",
    "reddit_dataset_iter = iter(reddit_dataset)\n",
    "# train_dataset = [next(reddit_dataset_iter) for _ in range(reddit_dataset.get_total() - args.test_size - args.eval_size -5200087)]\n",
    "\n",
    "# print(reddit_dataset.get_total())\n",
    "# reddit_dataset.get_total() - args.eval_size-100000\n",
    "train_dataset = [next(reddit_dataset_iter) for _ in range(470000)] #5200087\n",
    "eval_dataset = [next(reddit_dataset_iter) for _ in range(args.eval_size)]\n",
    "del reddit_dataset\n",
    "del reddit_dataset_iter\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"Target:\", eval_dataset[50][0])\n",
    "print(\"Input:\", eval_dataset[50][1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: Just like last year, we are not doing any April Fool's day jokes, nor are we allowing them. Please do not submit anything like that. We are also not doing a regular AMA (because it would not be fair to a guest to do an AMA on April first.) We are taking this opportunity to have a discussion with the community. What are we doing right or wrong? How could we make /r/science better? Ask us anything.\n",
      "Input: If the comment section remains active it doesn't get archived, that's how that longest comment post keeps rolling.\n"
     ]
    }
   ],
   "source": [
    "print(\"Target:\", eval_dataset[51][0])\n",
    "print(\"Input:\", eval_dataset[51][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/12244466/ipykernel_16745/235063363.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model_inputs[\"labels\"] = torch.tensor(model_inputs.data[\"input_ids\"]).to(device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='916' max='916' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [916/916 03:09, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.558043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.987300</td>\n",
       "      <td>3.560935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.987300</td>\n",
       "      <td>3.560512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./reddit-model/tokenizer_config.json',\n",
       " './reddit-model/special_tokens_map.json',\n",
       " './reddit-model/vocab.json',\n",
       " './reddit-model/merges.txt',\n",
       " './reddit-model/added_tokens.json',\n",
       " './reddit-model/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir_main,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "#     save_total_limit=2,\n",
    "    learning_rate=1e-5,\n",
    "    # fp16=True,\n",
    "    per_device_train_batch_size=256,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=256,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    hub_strategy=\"every_save\",\n",
    "    save_strategy=\"no\",\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=\"skunusot/gpt2-reddit\",\n",
    "    hub_private_repo=True,\n",
    "    hub_token=\"whf_LbwUQBNXqnUndGiCJePZLvNzcVRQCOXtSI\",\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# # implement the compute matrix to be used in evaluation the model\n",
    "# def compute_metrics(pred):\n",
    "#     labels_ids = pred.label_ids\n",
    "#     pred_ids = pred.predictions\n",
    "\n",
    "#     # all unnecessary tokens are removed\n",
    "#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "#     label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "#     rouge_score = rouge.compute(predictions=pred_str, references=label_str),\n",
    "\n",
    "#     return {\"bleu\": round(corpus_bleu(pred_str, [label_str]).score, 4),\n",
    "#             \"rouge\": rouge_score,\n",
    "#            }\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(args.model_name).to(device)\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "# # trainer.push_to_hub()\n",
    "\n",
    "trainer.save_model(output_dir_main)\n",
    "tokenizer.save_pretrained(output_dir_main)\n",
    "# compute_scores(test_dataset)\n",
    "\n",
    "# def compute_scores(test_dataset):\n",
    "#     tokenizer = GPT2Tokenizer.from_pretrained(output_dir_main)\n",
    "#     model = GPT2LMHeadModel.from_pretrained(output_dir_main)\n",
    "#     model.eval()\n",
    "#     test_input_ids = tokenizer.batch_encode_plus(test_dataset, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(test_input_ids, labels=test_input_ids)\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#     # Compute the perplexity as the ex\n",
    "#     ponential of the cross-entropy loss\n",
    "#     perplexity = math.exp(loss)\n",
    "#     print(f\"Perplexity: {perplexity:.2f}\")    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaultion = trainer.evaluate()\n",
    "print(evaultion)\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, StoppingCriteriaList, MaxLengthCriteria\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=64)])\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, length=200, temperature=1.0):\n",
    "    inp = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inp[\"input_ids\"]\n",
    "    a = inp[\"attention_mask\"]\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=a,\n",
    "        do_sample=True,\n",
    "#         max_length=length + len(input_ids[0]),\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        # repetition_penalty=1.2,\n",
    "        # repetition_penalty_range=512,\n",
    "        # repetition_penalty_slope=3.33,\n",
    "        pad_token_id=50256,\n",
    "        eos_token_id=50256\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(output_dir_main)\n",
    "# model = GPT2LMHeadModel.from_pretrained(output_dir_main)\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "# model = AutoModelWithLMHead.from_pretrained(args.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir_main)\n",
    "model = AutoModelWithLMHead.from_pretrained(output_dir_main)\n",
    "\n",
    "model.eval()\n",
    "# model = torch.load(model_path)\n",
    "prompt = \"How to use Excel?\"\n",
    "# prompt = \"What do you know about ethics?\"\n",
    "# prompt = \"Do we need to punish people for reckless driving?\"\n",
    "# prompt =\"Do you know Thomas Massie?\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inp = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inp[\"input_ids\"]\n",
    "a = inp[\"attention_mask\"]\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "output = model.generate(input_ids=input_ids, attention_mask=a, max_new_tokens = 150,  do_sample=True,pad_token_id=50256,\n",
    "        eos_token_id=50256)\n",
    "generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_sentence)\n",
    "\n",
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=a,\n",
    "    num_beams=5, \n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=5, \n",
    "#     max_length=50 + len(input_ids[0]),\n",
    "#     temperature=1,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cuda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
